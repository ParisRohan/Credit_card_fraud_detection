{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Data Collection","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-27T08:04:15.289572Z","iopub.execute_input":"2022-05-27T08:04:15.289971Z","iopub.status.idle":"2022-05-27T08:04:15.324605Z","shell.execute_reply.started":"2022-05-27T08:04:15.289867Z","shell.execute_reply":"2022-05-27T08:04:15.323808Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/creditcardfraud/creditcard.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"seaborn-whitegrid\")\n%matplotlib inline\n\n#model building libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost\n\n#library for resampling\nfrom imblearn.over_sampling import SMOTE\n\n#librares to check model performance\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n\n#libraries to perform hyperparameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#import library used for counting the number of observations\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.pandas.set_option('display.max_columns',None)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:20:29.955644Z","iopub.execute_input":"2022-05-27T09:20:29.955942Z","iopub.status.idle":"2022-05-27T09:20:30.103276Z","shell.execute_reply.started":"2022-05-27T09:20:29.955912Z","shell.execute_reply":"2022-05-27T09:20:30.102140Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#load the dataset\ndf_data = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\ndf_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:08:31.764695Z","iopub.execute_input":"2022-05-27T08:08:31.765034Z","iopub.status.idle":"2022-05-27T08:08:36.893490Z","shell.execute_reply.started":"2022-05-27T08:08:31.764999Z","shell.execute_reply":"2022-05-27T08:08:36.892283Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9       V10       V11       V12       V13       V14  \\\n0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n\n        V15       V16       V17       V18       V19       V20       V21  \\\n0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n\n        V22       V23       V24       V25       V26       V27       V28  \\\n0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n\n   Amount  Class  \n0  149.62      0  \n1    2.69      0  \n2  378.66      0  \n3  123.50      0  \n4   69.99      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>V11</th>\n      <th>V12</th>\n      <th>V13</th>\n      <th>V14</th>\n      <th>V15</th>\n      <th>V16</th>\n      <th>V17</th>\n      <th>V18</th>\n      <th>V19</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>0.090794</td>\n      <td>-0.551600</td>\n      <td>-0.617801</td>\n      <td>-0.991390</td>\n      <td>-0.311169</td>\n      <td>1.468177</td>\n      <td>-0.470401</td>\n      <td>0.207971</td>\n      <td>0.025791</td>\n      <td>0.403993</td>\n      <td>0.251412</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>-0.166974</td>\n      <td>1.612727</td>\n      <td>1.065235</td>\n      <td>0.489095</td>\n      <td>-0.143772</td>\n      <td>0.635558</td>\n      <td>0.463917</td>\n      <td>-0.114805</td>\n      <td>-0.183361</td>\n      <td>-0.145783</td>\n      <td>-0.069083</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>0.207643</td>\n      <td>0.624501</td>\n      <td>0.066084</td>\n      <td>0.717293</td>\n      <td>-0.165946</td>\n      <td>2.345865</td>\n      <td>-2.890083</td>\n      <td>1.109969</td>\n      <td>-0.121359</td>\n      <td>-2.261857</td>\n      <td>0.524980</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>-0.054952</td>\n      <td>-0.226487</td>\n      <td>0.178228</td>\n      <td>0.507757</td>\n      <td>-0.287924</td>\n      <td>-0.631418</td>\n      <td>-1.059647</td>\n      <td>-0.684093</td>\n      <td>1.965775</td>\n      <td>-1.232622</td>\n      <td>-0.208038</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>0.753074</td>\n      <td>-0.822843</td>\n      <td>0.538196</td>\n      <td>1.345852</td>\n      <td>-1.119670</td>\n      <td>0.175121</td>\n      <td>-0.451449</td>\n      <td>-0.237033</td>\n      <td>-0.038195</td>\n      <td>0.803487</td>\n      <td>0.408542</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Feature Description\n* Time: Number of seconds elapsed between this transaction and the first transaction in the dataset\n* V1 to V28: Sensitive data masked using PCA\n* Amount: Transaction amount\n* Class: 1 for fraudulent transactions, 0 otherwise -> TARGET feature","metadata":{}},{"cell_type":"markdown","source":"## Initial data analysis","metadata":{}},{"cell_type":"code","source":"df_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:09:18.786650Z","iopub.execute_input":"2022-05-27T08:09:18.787556Z","iopub.status.idle":"2022-05-27T08:09:18.795805Z","shell.execute_reply.started":"2022-05-27T08:09:18.787479Z","shell.execute_reply":"2022-05-27T08:09:18.794378Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(284807, 31)"},"metadata":{}}]},{"cell_type":"markdown","source":"**Observation:** The dataset has 284807 rows and 31 columns","metadata":{}},{"cell_type":"code","source":"df_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:09:56.441159Z","iopub.execute_input":"2022-05-27T08:09:56.442289Z","iopub.status.idle":"2022-05-27T08:09:56.497493Z","shell.execute_reply.started":"2022-05-27T08:09:56.442227Z","shell.execute_reply":"2022-05-27T08:09:56.496305Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"#check number of missing values\ndf_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:14:25.196422Z","iopub.execute_input":"2022-05-27T08:14:25.197078Z","iopub.status.idle":"2022-05-27T08:14:25.224659Z","shell.execute_reply.started":"2022-05-27T08:14:25.197038Z","shell.execute_reply":"2022-05-27T08:14:25.223553Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Time      0\nV1        0\nV2        0\nV3        0\nV4        0\nV5        0\nV6        0\nV7        0\nV8        0\nV9        0\nV10       0\nV11       0\nV12       0\nV13       0\nV14       0\nV15       0\nV16       0\nV17       0\nV18       0\nV19       0\nV20       0\nV21       0\nV22       0\nV23       0\nV24       0\nV25       0\nV26       0\nV27       0\nV28       0\nAmount    0\nClass     0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"**Observation:** The dataset has no missing values","metadata":{}},{"cell_type":"code","source":"#Check the number of transactions with 0 amount\ndf_data[df_data['Amount']==0]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:17:39.799105Z","iopub.execute_input":"2022-05-27T08:17:39.799429Z","iopub.status.idle":"2022-05-27T08:17:39.847785Z","shell.execute_reply.started":"2022-05-27T08:17:39.799391Z","shell.execute_reply":"2022-05-27T08:17:39.846948Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"            Time        V1        V2        V3        V4        V5        V6  \\\n383        282.0 -0.356466  0.725418  1.971749  0.831343  0.369681 -0.107776   \n514        380.0 -1.299837  0.881817  1.452842 -1.293698 -0.025105 -1.170103   \n534        403.0  1.237413  0.512365  0.687746  1.693872 -0.236323 -0.650232   \n541        406.0 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n575        430.0 -1.860258 -0.629859  0.966570  0.844632  0.759983 -1.481173   \n...          ...       ...       ...       ...       ...       ...       ...   \n283719  171817.0 -0.750414  0.904175  0.996461  0.427284  1.720336  0.929256   \n283782  171870.0  2.083677 -0.065811 -1.442870  0.135416  0.043035 -1.306975   \n283949  172027.0  2.132569 -0.057836 -1.724522 -0.030326  0.412146 -0.903088   \n284085  172140.0 -2.210521 -1.039425  0.189704 -1.291932  3.742120 -1.665061   \n284770  172759.0 -0.822731  1.270140 -0.138566  0.479620  1.242101  0.795218   \n\n              V7        V8        V9       V10       V11       V12       V13  \\\n383     0.751610 -0.120166 -0.420675 -0.059943 -0.508270  0.425506  0.414309   \n514     0.861610 -0.193934  0.592001  0.241979  0.144973 -0.583891 -1.595345   \n534     0.118066 -0.230545 -0.808523  0.511284 -0.178159  0.762909  1.700923   \n541    -2.537387  1.391657 -2.770089 -2.772272  3.202033 -2.899907 -0.595222   \n575    -0.509681  0.540722 -0.733623 -0.371622  0.859741  0.372609 -1.240185   \n...          ...       ...       ...       ...       ...       ...       ...   \n283719  0.794272  0.176719 -1.836261  0.233928 -0.569993  0.110682  0.784652   \n283782  0.335835 -0.371635  0.730560 -0.106473 -0.820816  0.268172 -0.431765   \n283949  0.345843 -0.348132  0.722638 -0.116179 -1.376166  0.234341  0.054018   \n284085  3.120388 -2.324089  0.364926  1.582486  1.099928  0.015128  0.071182   \n284770  0.454284  0.556038 -1.550610  0.523338 -0.779549  0.365876  0.880299   \n\n             V14       V15       V16       V17       V18       V19       V20  \\\n383    -0.698375 -1.465349 -0.119009 -0.144735 -1.332221 -1.547440 -0.133602   \n514     0.032613  0.752834 -0.094286 -0.163427 -1.111176 -1.124025  0.065979   \n534    -0.133861  0.402418  0.684668 -0.609395 -0.704277 -0.720726 -0.006716   \n541    -4.289254  0.389724 -1.140747 -2.830056 -0.016822  0.416956  0.126911   \n575     0.998391 -0.346387 -0.391679  0.348289  0.282125  1.165893  0.320450   \n...          ...       ...       ...       ...       ...       ...       ...   \n283719 -0.261719 -2.370735  0.943657 -0.923311 -0.107890  0.044242  0.116340   \n283782  0.508677  0.186750 -0.518524 -0.147862 -0.728870  0.234239 -0.278137   \n283949  0.394647  0.197209 -0.334269 -0.419726 -0.548358  0.492017 -0.232386   \n284085 -1.056637 -1.611600 -0.609454 -1.876089 -0.411326 -0.556234 -0.818118   \n284770 -0.004927 -2.585488  1.064460 -0.862512 -0.081841 -0.108819 -0.010139   \n\n             V21       V22       V23       V24       V25       V26       V27  \\\n383     0.020804  0.424312 -0.015989  0.466754 -0.809962  0.657334 -0.043150   \n514    -0.272563 -0.360853  0.223911  0.598930 -0.397705  0.637141  0.234872   \n534    -0.077543 -0.178220  0.038722  0.471218  0.289249  0.871803 -0.066884   \n541     0.517232 -0.035049 -0.465211  0.320198  0.044519  0.177840  0.261145   \n575     0.268028  0.125515 -0.225029  0.586664 -0.031598  0.570168 -0.043007   \n...          ...       ...       ...       ...       ...       ...       ...   \n283719  0.050750  0.115532 -0.623995 -0.186896  0.733759  2.558151 -0.188835   \n283782 -0.147536 -0.246599  0.194758 -0.082277  0.012887 -0.069278 -0.048995   \n283949 -0.188739 -0.343876  0.105024 -0.763831  0.117381 -0.027682 -0.047514   \n284085 -0.286359  1.326003 -0.361764 -0.268117  1.051309  0.334629 -1.930149   \n284770  0.138766  0.450908 -0.192146 -0.196218 -0.261664  2.372675 -0.042743   \n\n             V28  Amount  Class  \n383    -0.046401     0.0      0  \n514     0.021379     0.0      0  \n534     0.012986     0.0      0  \n541    -0.143276     0.0      1  \n575    -0.223739     0.0      0  \n...          ...     ...    ...  \n283719  0.001654     0.0      0  \n283782 -0.065482     0.0      0  \n283949 -0.071700     0.0      0  \n284085 -0.899888     0.0      0  \n284770  0.109613     0.0      0  \n\n[1825 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>V11</th>\n      <th>V12</th>\n      <th>V13</th>\n      <th>V14</th>\n      <th>V15</th>\n      <th>V16</th>\n      <th>V17</th>\n      <th>V18</th>\n      <th>V19</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>383</th>\n      <td>282.0</td>\n      <td>-0.356466</td>\n      <td>0.725418</td>\n      <td>1.971749</td>\n      <td>0.831343</td>\n      <td>0.369681</td>\n      <td>-0.107776</td>\n      <td>0.751610</td>\n      <td>-0.120166</td>\n      <td>-0.420675</td>\n      <td>-0.059943</td>\n      <td>-0.508270</td>\n      <td>0.425506</td>\n      <td>0.414309</td>\n      <td>-0.698375</td>\n      <td>-1.465349</td>\n      <td>-0.119009</td>\n      <td>-0.144735</td>\n      <td>-1.332221</td>\n      <td>-1.547440</td>\n      <td>-0.133602</td>\n      <td>0.020804</td>\n      <td>0.424312</td>\n      <td>-0.015989</td>\n      <td>0.466754</td>\n      <td>-0.809962</td>\n      <td>0.657334</td>\n      <td>-0.043150</td>\n      <td>-0.046401</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>514</th>\n      <td>380.0</td>\n      <td>-1.299837</td>\n      <td>0.881817</td>\n      <td>1.452842</td>\n      <td>-1.293698</td>\n      <td>-0.025105</td>\n      <td>-1.170103</td>\n      <td>0.861610</td>\n      <td>-0.193934</td>\n      <td>0.592001</td>\n      <td>0.241979</td>\n      <td>0.144973</td>\n      <td>-0.583891</td>\n      <td>-1.595345</td>\n      <td>0.032613</td>\n      <td>0.752834</td>\n      <td>-0.094286</td>\n      <td>-0.163427</td>\n      <td>-1.111176</td>\n      <td>-1.124025</td>\n      <td>0.065979</td>\n      <td>-0.272563</td>\n      <td>-0.360853</td>\n      <td>0.223911</td>\n      <td>0.598930</td>\n      <td>-0.397705</td>\n      <td>0.637141</td>\n      <td>0.234872</td>\n      <td>0.021379</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>534</th>\n      <td>403.0</td>\n      <td>1.237413</td>\n      <td>0.512365</td>\n      <td>0.687746</td>\n      <td>1.693872</td>\n      <td>-0.236323</td>\n      <td>-0.650232</td>\n      <td>0.118066</td>\n      <td>-0.230545</td>\n      <td>-0.808523</td>\n      <td>0.511284</td>\n      <td>-0.178159</td>\n      <td>0.762909</td>\n      <td>1.700923</td>\n      <td>-0.133861</td>\n      <td>0.402418</td>\n      <td>0.684668</td>\n      <td>-0.609395</td>\n      <td>-0.704277</td>\n      <td>-0.720726</td>\n      <td>-0.006716</td>\n      <td>-0.077543</td>\n      <td>-0.178220</td>\n      <td>0.038722</td>\n      <td>0.471218</td>\n      <td>0.289249</td>\n      <td>0.871803</td>\n      <td>-0.066884</td>\n      <td>0.012986</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>541</th>\n      <td>406.0</td>\n      <td>-2.312227</td>\n      <td>1.951992</td>\n      <td>-1.609851</td>\n      <td>3.997906</td>\n      <td>-0.522188</td>\n      <td>-1.426545</td>\n      <td>-2.537387</td>\n      <td>1.391657</td>\n      <td>-2.770089</td>\n      <td>-2.772272</td>\n      <td>3.202033</td>\n      <td>-2.899907</td>\n      <td>-0.595222</td>\n      <td>-4.289254</td>\n      <td>0.389724</td>\n      <td>-1.140747</td>\n      <td>-2.830056</td>\n      <td>-0.016822</td>\n      <td>0.416956</td>\n      <td>0.126911</td>\n      <td>0.517232</td>\n      <td>-0.035049</td>\n      <td>-0.465211</td>\n      <td>0.320198</td>\n      <td>0.044519</td>\n      <td>0.177840</td>\n      <td>0.261145</td>\n      <td>-0.143276</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>575</th>\n      <td>430.0</td>\n      <td>-1.860258</td>\n      <td>-0.629859</td>\n      <td>0.966570</td>\n      <td>0.844632</td>\n      <td>0.759983</td>\n      <td>-1.481173</td>\n      <td>-0.509681</td>\n      <td>0.540722</td>\n      <td>-0.733623</td>\n      <td>-0.371622</td>\n      <td>0.859741</td>\n      <td>0.372609</td>\n      <td>-1.240185</td>\n      <td>0.998391</td>\n      <td>-0.346387</td>\n      <td>-0.391679</td>\n      <td>0.348289</td>\n      <td>0.282125</td>\n      <td>1.165893</td>\n      <td>0.320450</td>\n      <td>0.268028</td>\n      <td>0.125515</td>\n      <td>-0.225029</td>\n      <td>0.586664</td>\n      <td>-0.031598</td>\n      <td>0.570168</td>\n      <td>-0.043007</td>\n      <td>-0.223739</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>283719</th>\n      <td>171817.0</td>\n      <td>-0.750414</td>\n      <td>0.904175</td>\n      <td>0.996461</td>\n      <td>0.427284</td>\n      <td>1.720336</td>\n      <td>0.929256</td>\n      <td>0.794272</td>\n      <td>0.176719</td>\n      <td>-1.836261</td>\n      <td>0.233928</td>\n      <td>-0.569993</td>\n      <td>0.110682</td>\n      <td>0.784652</td>\n      <td>-0.261719</td>\n      <td>-2.370735</td>\n      <td>0.943657</td>\n      <td>-0.923311</td>\n      <td>-0.107890</td>\n      <td>0.044242</td>\n      <td>0.116340</td>\n      <td>0.050750</td>\n      <td>0.115532</td>\n      <td>-0.623995</td>\n      <td>-0.186896</td>\n      <td>0.733759</td>\n      <td>2.558151</td>\n      <td>-0.188835</td>\n      <td>0.001654</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>283782</th>\n      <td>171870.0</td>\n      <td>2.083677</td>\n      <td>-0.065811</td>\n      <td>-1.442870</td>\n      <td>0.135416</td>\n      <td>0.043035</td>\n      <td>-1.306975</td>\n      <td>0.335835</td>\n      <td>-0.371635</td>\n      <td>0.730560</td>\n      <td>-0.106473</td>\n      <td>-0.820816</td>\n      <td>0.268172</td>\n      <td>-0.431765</td>\n      <td>0.508677</td>\n      <td>0.186750</td>\n      <td>-0.518524</td>\n      <td>-0.147862</td>\n      <td>-0.728870</td>\n      <td>0.234239</td>\n      <td>-0.278137</td>\n      <td>-0.147536</td>\n      <td>-0.246599</td>\n      <td>0.194758</td>\n      <td>-0.082277</td>\n      <td>0.012887</td>\n      <td>-0.069278</td>\n      <td>-0.048995</td>\n      <td>-0.065482</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>283949</th>\n      <td>172027.0</td>\n      <td>2.132569</td>\n      <td>-0.057836</td>\n      <td>-1.724522</td>\n      <td>-0.030326</td>\n      <td>0.412146</td>\n      <td>-0.903088</td>\n      <td>0.345843</td>\n      <td>-0.348132</td>\n      <td>0.722638</td>\n      <td>-0.116179</td>\n      <td>-1.376166</td>\n      <td>0.234341</td>\n      <td>0.054018</td>\n      <td>0.394647</td>\n      <td>0.197209</td>\n      <td>-0.334269</td>\n      <td>-0.419726</td>\n      <td>-0.548358</td>\n      <td>0.492017</td>\n      <td>-0.232386</td>\n      <td>-0.188739</td>\n      <td>-0.343876</td>\n      <td>0.105024</td>\n      <td>-0.763831</td>\n      <td>0.117381</td>\n      <td>-0.027682</td>\n      <td>-0.047514</td>\n      <td>-0.071700</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284085</th>\n      <td>172140.0</td>\n      <td>-2.210521</td>\n      <td>-1.039425</td>\n      <td>0.189704</td>\n      <td>-1.291932</td>\n      <td>3.742120</td>\n      <td>-1.665061</td>\n      <td>3.120388</td>\n      <td>-2.324089</td>\n      <td>0.364926</td>\n      <td>1.582486</td>\n      <td>1.099928</td>\n      <td>0.015128</td>\n      <td>0.071182</td>\n      <td>-1.056637</td>\n      <td>-1.611600</td>\n      <td>-0.609454</td>\n      <td>-1.876089</td>\n      <td>-0.411326</td>\n      <td>-0.556234</td>\n      <td>-0.818118</td>\n      <td>-0.286359</td>\n      <td>1.326003</td>\n      <td>-0.361764</td>\n      <td>-0.268117</td>\n      <td>1.051309</td>\n      <td>0.334629</td>\n      <td>-1.930149</td>\n      <td>-0.899888</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284770</th>\n      <td>172759.0</td>\n      <td>-0.822731</td>\n      <td>1.270140</td>\n      <td>-0.138566</td>\n      <td>0.479620</td>\n      <td>1.242101</td>\n      <td>0.795218</td>\n      <td>0.454284</td>\n      <td>0.556038</td>\n      <td>-1.550610</td>\n      <td>0.523338</td>\n      <td>-0.779549</td>\n      <td>0.365876</td>\n      <td>0.880299</td>\n      <td>-0.004927</td>\n      <td>-2.585488</td>\n      <td>1.064460</td>\n      <td>-0.862512</td>\n      <td>-0.081841</td>\n      <td>-0.108819</td>\n      <td>-0.010139</td>\n      <td>0.138766</td>\n      <td>0.450908</td>\n      <td>-0.192146</td>\n      <td>-0.196218</td>\n      <td>-0.261664</td>\n      <td>2.372675</td>\n      <td>-0.042743</td>\n      <td>0.109613</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1825 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Check the number of transactions with negative amount\ndf_data[df_data['Amount']<0]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:18:25.511145Z","iopub.execute_input":"2022-05-27T08:18:25.511619Z","iopub.status.idle":"2022-05-27T08:18:25.529100Z","shell.execute_reply.started":"2022-05-27T08:18:25.511568Z","shell.execute_reply":"2022-05-27T08:18:25.528479Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount, Class]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>V11</th>\n      <th>V12</th>\n      <th>V13</th>\n      <th>V14</th>\n      <th>V15</th>\n      <th>V16</th>\n      <th>V17</th>\n      <th>V18</th>\n      <th>V19</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Check the data distribution of Target feature\nsns.countplot(x=\"Class\", data=df_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:18:54.207043Z","iopub.execute_input":"2022-05-27T08:18:54.207924Z","iopub.status.idle":"2022-05-27T08:18:54.406873Z","shell.execute_reply.started":"2022-05-27T08:18:54.207867Z","shell.execute_reply":"2022-05-27T08:18:54.405481Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:xlabel='Class', ylabel='count'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"df_data['Class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T08:19:10.419920Z","iopub.execute_input":"2022-05-27T08:19:10.420261Z","iopub.status.idle":"2022-05-27T08:19:10.429227Z","shell.execute_reply.started":"2022-05-27T08:19:10.420229Z","shell.execute_reply":"2022-05-27T08:19:10.428651Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0    284315\n1       492\nName: Class, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"**Observation:** \n* This is an **imbalanced data** as the number of '0-Legit transactions' is far greater than the number of '1-Fraud transactions' in our dataset\n* 99.82% data is for '0' and remaining data is for '1'","metadata":{}},{"cell_type":"code","source":"df_data[df_data['Class']==0].describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:08:35.966722Z","iopub.execute_input":"2022-05-27T09:08:35.967002Z","iopub.status.idle":"2022-05-27T09:08:36.521666Z","shell.execute_reply.started":"2022-05-27T09:08:35.966974Z","shell.execute_reply":"2022-05-27T09:08:36.520412Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                Time             V1             V2             V3  \\\ncount  284315.000000  284315.000000  284315.000000  284315.000000   \nmean    94838.202258       0.008258      -0.006271       0.012171   \nstd     47484.015786       1.929814       1.636146       1.459429   \nmin         0.000000     -56.407510     -72.715728     -48.325589   \n25%     54230.000000      -0.917544      -0.599473      -0.884541   \n50%     84711.000000       0.020023       0.064070       0.182158   \n75%    139333.000000       1.316218       0.800446       1.028372   \nmax    172792.000000       2.454930      18.902453       9.382558   \n\n                  V4             V5             V6             V7  \\\ncount  284315.000000  284315.000000  284315.000000  284315.000000   \nmean       -0.007860       0.005453       0.002419       0.009637   \nstd         1.399333       1.356952       1.329913       1.178812   \nmin        -5.683171    -113.743307     -26.160506     -31.764946   \n25%        -0.850077      -0.689398      -0.766847      -0.551442   \n50%        -0.022405      -0.053457      -0.273123       0.041138   \n75%         0.737624       0.612181       0.399619       0.571019   \nmax        16.875344      34.801666      73.301626     120.589494   \n\n                  V8             V9            V10            V11  \\\ncount  284315.000000  284315.000000  284315.000000  284315.000000   \nmean       -0.000987       0.004467       0.009824      -0.006576   \nstd         1.161283       1.089372       1.044204       1.003112   \nmin       -73.216718      -6.290730     -14.741096      -4.797473   \n25%        -0.208633      -0.640412      -0.532880      -0.763447   \n50%         0.022041      -0.049964      -0.091872      -0.034923   \n75%         0.326200       0.598230       0.455135       0.736362   \nmax        18.709255      15.594995      23.745136      10.002190   \n\n                 V12            V13            V14            V15  \\\ncount  284315.000000  284315.000000  284315.000000  284315.000000   \nmean        0.010832       0.000189       0.012064       0.000161   \nstd         0.945939       0.995067       0.897007       0.915060   \nmin       -15.144988      -5.791881     -18.392091      -4.391307   \n25%        -0.402102      -0.648067      -0.422453      -0.582812   \n50%         0.141679      -0.013547       0.051947       0.048294   \n75%         0.619207       0.662492       0.494104       0.648842   \nmax         7.848392       7.126883      10.526766       8.877742   \n\n                 V16            V17            V18            V19  \\\ncount  284315.000000  284315.000000  284315.000000  284315.000000   \nmean        0.007164       0.011535       0.003887      -0.001178   \nstd         0.844772       0.749457       0.824919       0.811733   \nmin       -10.115560     -17.098444      -5.366660      -7.213527   \n25%        -0.465543      -0.482644      -0.497414      -0.456366   \n50%         0.067377      -0.064833      -0.002787       0.003117   \n75%         0.523738       0.399922       0.501103       0.457499   \nmax        17.315112       9.253526       5.041069       5.591971   \n\n                 V20            V21            V22            V23  \\\ncount  284315.000000  284315.000000  284315.000000  284315.000000   \nmean       -0.000644      -0.001235      -0.000024       0.000070   \nstd         0.769404       0.716743       0.723668       0.621541   \nmin       -54.497720     -34.830382     -10.933144     -44.807735   \n25%        -0.211764      -0.228509      -0.542403      -0.161702   \n50%        -0.062646      -0.029821       0.006736      -0.011147   \n75%         0.132401       0.185626       0.528407       0.147522   \nmax        39.420904      22.614889      10.503090      22.528412   \n\n                 V24            V25            V26            V27  \\\ncount  284315.000000  284315.000000  284315.000000  284315.000000   \nmean        0.000182      -0.000072      -0.000089      -0.000295   \nstd         0.605776       0.520673       0.482241       0.399847   \nmin        -2.836627     -10.295397      -2.604551     -22.565679   \n25%        -0.354425      -0.317145      -0.327074      -0.070852   \n50%         0.041082       0.016417      -0.052227       0.001230   \n75%         0.439869       0.350594       0.240671       0.090573   \nmax         4.584549       7.519589       3.517346      31.612198   \n\n                 V28         Amount     Class  \ncount  284315.000000  284315.000000  284315.0  \nmean       -0.000131      88.291022       0.0  \nstd         0.329570     250.105092       0.0  \nmin       -15.430084       0.000000       0.0  \n25%        -0.052950       5.650000       0.0  \n50%         0.011199      22.000000       0.0  \n75%         0.077962      77.050000       0.0  \nmax        33.847808   25691.160000       0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>V11</th>\n      <th>V12</th>\n      <th>V13</th>\n      <th>V14</th>\n      <th>V15</th>\n      <th>V16</th>\n      <th>V17</th>\n      <th>V18</th>\n      <th>V19</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.000000</td>\n      <td>284315.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>94838.202258</td>\n      <td>0.008258</td>\n      <td>-0.006271</td>\n      <td>0.012171</td>\n      <td>-0.007860</td>\n      <td>0.005453</td>\n      <td>0.002419</td>\n      <td>0.009637</td>\n      <td>-0.000987</td>\n      <td>0.004467</td>\n      <td>0.009824</td>\n      <td>-0.006576</td>\n      <td>0.010832</td>\n      <td>0.000189</td>\n      <td>0.012064</td>\n      <td>0.000161</td>\n      <td>0.007164</td>\n      <td>0.011535</td>\n      <td>0.003887</td>\n      <td>-0.001178</td>\n      <td>-0.000644</td>\n      <td>-0.001235</td>\n      <td>-0.000024</td>\n      <td>0.000070</td>\n      <td>0.000182</td>\n      <td>-0.000072</td>\n      <td>-0.000089</td>\n      <td>-0.000295</td>\n      <td>-0.000131</td>\n      <td>88.291022</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>47484.015786</td>\n      <td>1.929814</td>\n      <td>1.636146</td>\n      <td>1.459429</td>\n      <td>1.399333</td>\n      <td>1.356952</td>\n      <td>1.329913</td>\n      <td>1.178812</td>\n      <td>1.161283</td>\n      <td>1.089372</td>\n      <td>1.044204</td>\n      <td>1.003112</td>\n      <td>0.945939</td>\n      <td>0.995067</td>\n      <td>0.897007</td>\n      <td>0.915060</td>\n      <td>0.844772</td>\n      <td>0.749457</td>\n      <td>0.824919</td>\n      <td>0.811733</td>\n      <td>0.769404</td>\n      <td>0.716743</td>\n      <td>0.723668</td>\n      <td>0.621541</td>\n      <td>0.605776</td>\n      <td>0.520673</td>\n      <td>0.482241</td>\n      <td>0.399847</td>\n      <td>0.329570</td>\n      <td>250.105092</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-56.407510</td>\n      <td>-72.715728</td>\n      <td>-48.325589</td>\n      <td>-5.683171</td>\n      <td>-113.743307</td>\n      <td>-26.160506</td>\n      <td>-31.764946</td>\n      <td>-73.216718</td>\n      <td>-6.290730</td>\n      <td>-14.741096</td>\n      <td>-4.797473</td>\n      <td>-15.144988</td>\n      <td>-5.791881</td>\n      <td>-18.392091</td>\n      <td>-4.391307</td>\n      <td>-10.115560</td>\n      <td>-17.098444</td>\n      <td>-5.366660</td>\n      <td>-7.213527</td>\n      <td>-54.497720</td>\n      <td>-34.830382</td>\n      <td>-10.933144</td>\n      <td>-44.807735</td>\n      <td>-2.836627</td>\n      <td>-10.295397</td>\n      <td>-2.604551</td>\n      <td>-22.565679</td>\n      <td>-15.430084</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>54230.000000</td>\n      <td>-0.917544</td>\n      <td>-0.599473</td>\n      <td>-0.884541</td>\n      <td>-0.850077</td>\n      <td>-0.689398</td>\n      <td>-0.766847</td>\n      <td>-0.551442</td>\n      <td>-0.208633</td>\n      <td>-0.640412</td>\n      <td>-0.532880</td>\n      <td>-0.763447</td>\n      <td>-0.402102</td>\n      <td>-0.648067</td>\n      <td>-0.422453</td>\n      <td>-0.582812</td>\n      <td>-0.465543</td>\n      <td>-0.482644</td>\n      <td>-0.497414</td>\n      <td>-0.456366</td>\n      <td>-0.211764</td>\n      <td>-0.228509</td>\n      <td>-0.542403</td>\n      <td>-0.161702</td>\n      <td>-0.354425</td>\n      <td>-0.317145</td>\n      <td>-0.327074</td>\n      <td>-0.070852</td>\n      <td>-0.052950</td>\n      <td>5.650000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>84711.000000</td>\n      <td>0.020023</td>\n      <td>0.064070</td>\n      <td>0.182158</td>\n      <td>-0.022405</td>\n      <td>-0.053457</td>\n      <td>-0.273123</td>\n      <td>0.041138</td>\n      <td>0.022041</td>\n      <td>-0.049964</td>\n      <td>-0.091872</td>\n      <td>-0.034923</td>\n      <td>0.141679</td>\n      <td>-0.013547</td>\n      <td>0.051947</td>\n      <td>0.048294</td>\n      <td>0.067377</td>\n      <td>-0.064833</td>\n      <td>-0.002787</td>\n      <td>0.003117</td>\n      <td>-0.062646</td>\n      <td>-0.029821</td>\n      <td>0.006736</td>\n      <td>-0.011147</td>\n      <td>0.041082</td>\n      <td>0.016417</td>\n      <td>-0.052227</td>\n      <td>0.001230</td>\n      <td>0.011199</td>\n      <td>22.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>139333.000000</td>\n      <td>1.316218</td>\n      <td>0.800446</td>\n      <td>1.028372</td>\n      <td>0.737624</td>\n      <td>0.612181</td>\n      <td>0.399619</td>\n      <td>0.571019</td>\n      <td>0.326200</td>\n      <td>0.598230</td>\n      <td>0.455135</td>\n      <td>0.736362</td>\n      <td>0.619207</td>\n      <td>0.662492</td>\n      <td>0.494104</td>\n      <td>0.648842</td>\n      <td>0.523738</td>\n      <td>0.399922</td>\n      <td>0.501103</td>\n      <td>0.457499</td>\n      <td>0.132401</td>\n      <td>0.185626</td>\n      <td>0.528407</td>\n      <td>0.147522</td>\n      <td>0.439869</td>\n      <td>0.350594</td>\n      <td>0.240671</td>\n      <td>0.090573</td>\n      <td>0.077962</td>\n      <td>77.050000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>172792.000000</td>\n      <td>2.454930</td>\n      <td>18.902453</td>\n      <td>9.382558</td>\n      <td>16.875344</td>\n      <td>34.801666</td>\n      <td>73.301626</td>\n      <td>120.589494</td>\n      <td>18.709255</td>\n      <td>15.594995</td>\n      <td>23.745136</td>\n      <td>10.002190</td>\n      <td>7.848392</td>\n      <td>7.126883</td>\n      <td>10.526766</td>\n      <td>8.877742</td>\n      <td>17.315112</td>\n      <td>9.253526</td>\n      <td>5.041069</td>\n      <td>5.591971</td>\n      <td>39.420904</td>\n      <td>22.614889</td>\n      <td>10.503090</td>\n      <td>22.528412</td>\n      <td>4.584549</td>\n      <td>7.519589</td>\n      <td>3.517346</td>\n      <td>31.612198</td>\n      <td>33.847808</td>\n      <td>25691.160000</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_data[df_data['Class']==1].describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:08:54.743592Z","iopub.execute_input":"2022-05-27T09:08:54.743934Z","iopub.status.idle":"2022-05-27T09:08:54.843835Z","shell.execute_reply.started":"2022-05-27T09:08:54.743899Z","shell.execute_reply":"2022-05-27T09:08:54.842747Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                Time          V1          V2          V3          V4  \\\ncount     492.000000  492.000000  492.000000  492.000000  492.000000   \nmean    80746.806911   -4.771948    3.623778   -7.033281    4.542029   \nstd     47835.365138    6.783687    4.291216    7.110937    2.873318   \nmin       406.000000  -30.552380   -8.402154  -31.103685   -1.313275   \n25%     41241.500000   -6.036063    1.188226   -8.643489    2.373050   \n50%     75568.500000   -2.342497    2.717869   -5.075257    4.177147   \n75%    128483.000000   -0.419200    4.971257   -2.276185    6.348729   \nmax    170348.000000    2.132386   22.057729    2.250210   12.114672   \n\n               V5          V6          V7          V8          V9         V10  \\\ncount  492.000000  492.000000  492.000000  492.000000  492.000000  492.000000   \nmean    -3.151225   -1.397737   -5.568731    0.570636   -2.581123   -5.676883   \nstd      5.372468    1.858124    7.206773    6.797831    2.500896    4.897341   \nmin    -22.105532   -6.406267  -43.557242  -41.044261  -13.434066  -24.588262   \n25%     -4.792835   -2.501511   -7.965295   -0.195336   -3.872383   -7.756698   \n50%     -1.522962   -1.424616   -3.034402    0.621508   -2.208768   -4.578825   \n75%      0.214562   -0.413216   -0.945954    1.764879   -0.787850   -2.614184   \nmax     11.095089    6.474115    5.802537   20.007208    3.353525    4.031435   \n\n              V11         V12         V13         V14         V15         V16  \\\ncount  492.000000  492.000000  492.000000  492.000000  492.000000  492.000000   \nmean     3.800173   -6.259393   -0.109334   -6.971723   -0.092929   -4.139946   \nstd      2.678605    4.654458    1.104518    4.278940    1.049915    3.865035   \nmin     -1.702228  -18.683715   -3.127795  -19.214325   -4.498945  -14.129855   \n25%      1.973397   -8.688177   -0.979117   -9.692723   -0.643539   -6.562915   \n50%      3.586218   -5.502530   -0.065566   -6.729720   -0.057227   -3.549795   \n75%      5.307078   -2.974088    0.672964   -4.282821    0.609189   -1.226043   \nmax     12.018913    1.375941    2.815440    3.442422    2.471358    3.139656   \n\n              V17         V18         V19         V20         V21         V22  \\\ncount  492.000000  492.000000  492.000000  492.000000  492.000000  492.000000   \nmean    -6.665836   -2.246308    0.680659    0.372319    0.713588    0.014049   \nstd      6.970618    2.899366    1.539853    1.346635    3.869304    1.494602   \nmin    -25.162799   -9.498746   -3.681904   -4.128186  -22.797604   -8.887017   \n25%    -11.945057   -4.664576   -0.299423   -0.171760    0.041787   -0.533764   \n50%     -5.302949   -1.664346    0.646807    0.284693    0.592146    0.048434   \n75%     -1.341940    0.091772    1.649318    0.822445    1.244611    0.617474   \nmax      6.739384    3.790316    5.228342   11.059004   27.202839    8.361985   \n\n              V23         V24         V25         V26         V27         V28  \\\ncount  492.000000  492.000000  492.000000  492.000000  492.000000  492.000000   \nmean    -0.040308   -0.105130    0.041449    0.051648    0.170575    0.075667   \nstd      1.579642    0.515577    0.797205    0.471679    1.376766    0.547291   \nmin    -19.254328   -2.028024   -4.781606   -1.152671   -7.263482   -1.869290   \n25%     -0.342175   -0.436809   -0.314348   -0.259416   -0.020025   -0.108868   \n50%     -0.073135   -0.060795    0.088371    0.004321    0.394926    0.146344   \n75%      0.308378    0.285328    0.456515    0.396733    0.826029    0.381152   \nmax      5.466230    1.091435    2.208209    2.745261    3.052358    1.779364   \n\n            Amount  Class  \ncount   492.000000  492.0  \nmean    122.211321    1.0  \nstd     256.683288    0.0  \nmin       0.000000    1.0  \n25%       1.000000    1.0  \n50%       9.250000    1.0  \n75%     105.890000    1.0  \nmax    2125.870000    1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>V11</th>\n      <th>V12</th>\n      <th>V13</th>\n      <th>V14</th>\n      <th>V15</th>\n      <th>V16</th>\n      <th>V17</th>\n      <th>V18</th>\n      <th>V19</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.000000</td>\n      <td>492.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>80746.806911</td>\n      <td>-4.771948</td>\n      <td>3.623778</td>\n      <td>-7.033281</td>\n      <td>4.542029</td>\n      <td>-3.151225</td>\n      <td>-1.397737</td>\n      <td>-5.568731</td>\n      <td>0.570636</td>\n      <td>-2.581123</td>\n      <td>-5.676883</td>\n      <td>3.800173</td>\n      <td>-6.259393</td>\n      <td>-0.109334</td>\n      <td>-6.971723</td>\n      <td>-0.092929</td>\n      <td>-4.139946</td>\n      <td>-6.665836</td>\n      <td>-2.246308</td>\n      <td>0.680659</td>\n      <td>0.372319</td>\n      <td>0.713588</td>\n      <td>0.014049</td>\n      <td>-0.040308</td>\n      <td>-0.105130</td>\n      <td>0.041449</td>\n      <td>0.051648</td>\n      <td>0.170575</td>\n      <td>0.075667</td>\n      <td>122.211321</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>47835.365138</td>\n      <td>6.783687</td>\n      <td>4.291216</td>\n      <td>7.110937</td>\n      <td>2.873318</td>\n      <td>5.372468</td>\n      <td>1.858124</td>\n      <td>7.206773</td>\n      <td>6.797831</td>\n      <td>2.500896</td>\n      <td>4.897341</td>\n      <td>2.678605</td>\n      <td>4.654458</td>\n      <td>1.104518</td>\n      <td>4.278940</td>\n      <td>1.049915</td>\n      <td>3.865035</td>\n      <td>6.970618</td>\n      <td>2.899366</td>\n      <td>1.539853</td>\n      <td>1.346635</td>\n      <td>3.869304</td>\n      <td>1.494602</td>\n      <td>1.579642</td>\n      <td>0.515577</td>\n      <td>0.797205</td>\n      <td>0.471679</td>\n      <td>1.376766</td>\n      <td>0.547291</td>\n      <td>256.683288</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>406.000000</td>\n      <td>-30.552380</td>\n      <td>-8.402154</td>\n      <td>-31.103685</td>\n      <td>-1.313275</td>\n      <td>-22.105532</td>\n      <td>-6.406267</td>\n      <td>-43.557242</td>\n      <td>-41.044261</td>\n      <td>-13.434066</td>\n      <td>-24.588262</td>\n      <td>-1.702228</td>\n      <td>-18.683715</td>\n      <td>-3.127795</td>\n      <td>-19.214325</td>\n      <td>-4.498945</td>\n      <td>-14.129855</td>\n      <td>-25.162799</td>\n      <td>-9.498746</td>\n      <td>-3.681904</td>\n      <td>-4.128186</td>\n      <td>-22.797604</td>\n      <td>-8.887017</td>\n      <td>-19.254328</td>\n      <td>-2.028024</td>\n      <td>-4.781606</td>\n      <td>-1.152671</td>\n      <td>-7.263482</td>\n      <td>-1.869290</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>41241.500000</td>\n      <td>-6.036063</td>\n      <td>1.188226</td>\n      <td>-8.643489</td>\n      <td>2.373050</td>\n      <td>-4.792835</td>\n      <td>-2.501511</td>\n      <td>-7.965295</td>\n      <td>-0.195336</td>\n      <td>-3.872383</td>\n      <td>-7.756698</td>\n      <td>1.973397</td>\n      <td>-8.688177</td>\n      <td>-0.979117</td>\n      <td>-9.692723</td>\n      <td>-0.643539</td>\n      <td>-6.562915</td>\n      <td>-11.945057</td>\n      <td>-4.664576</td>\n      <td>-0.299423</td>\n      <td>-0.171760</td>\n      <td>0.041787</td>\n      <td>-0.533764</td>\n      <td>-0.342175</td>\n      <td>-0.436809</td>\n      <td>-0.314348</td>\n      <td>-0.259416</td>\n      <td>-0.020025</td>\n      <td>-0.108868</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>75568.500000</td>\n      <td>-2.342497</td>\n      <td>2.717869</td>\n      <td>-5.075257</td>\n      <td>4.177147</td>\n      <td>-1.522962</td>\n      <td>-1.424616</td>\n      <td>-3.034402</td>\n      <td>0.621508</td>\n      <td>-2.208768</td>\n      <td>-4.578825</td>\n      <td>3.586218</td>\n      <td>-5.502530</td>\n      <td>-0.065566</td>\n      <td>-6.729720</td>\n      <td>-0.057227</td>\n      <td>-3.549795</td>\n      <td>-5.302949</td>\n      <td>-1.664346</td>\n      <td>0.646807</td>\n      <td>0.284693</td>\n      <td>0.592146</td>\n      <td>0.048434</td>\n      <td>-0.073135</td>\n      <td>-0.060795</td>\n      <td>0.088371</td>\n      <td>0.004321</td>\n      <td>0.394926</td>\n      <td>0.146344</td>\n      <td>9.250000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>128483.000000</td>\n      <td>-0.419200</td>\n      <td>4.971257</td>\n      <td>-2.276185</td>\n      <td>6.348729</td>\n      <td>0.214562</td>\n      <td>-0.413216</td>\n      <td>-0.945954</td>\n      <td>1.764879</td>\n      <td>-0.787850</td>\n      <td>-2.614184</td>\n      <td>5.307078</td>\n      <td>-2.974088</td>\n      <td>0.672964</td>\n      <td>-4.282821</td>\n      <td>0.609189</td>\n      <td>-1.226043</td>\n      <td>-1.341940</td>\n      <td>0.091772</td>\n      <td>1.649318</td>\n      <td>0.822445</td>\n      <td>1.244611</td>\n      <td>0.617474</td>\n      <td>0.308378</td>\n      <td>0.285328</td>\n      <td>0.456515</td>\n      <td>0.396733</td>\n      <td>0.826029</td>\n      <td>0.381152</td>\n      <td>105.890000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>170348.000000</td>\n      <td>2.132386</td>\n      <td>22.057729</td>\n      <td>2.250210</td>\n      <td>12.114672</td>\n      <td>11.095089</td>\n      <td>6.474115</td>\n      <td>5.802537</td>\n      <td>20.007208</td>\n      <td>3.353525</td>\n      <td>4.031435</td>\n      <td>12.018913</td>\n      <td>1.375941</td>\n      <td>2.815440</td>\n      <td>3.442422</td>\n      <td>2.471358</td>\n      <td>3.139656</td>\n      <td>6.739384</td>\n      <td>3.790316</td>\n      <td>5.228342</td>\n      <td>11.059004</td>\n      <td>27.202839</td>\n      <td>8.361985</td>\n      <td>5.466230</td>\n      <td>1.091435</td>\n      <td>2.208209</td>\n      <td>2.745261</td>\n      <td>3.052358</td>\n      <td>1.779364</td>\n      <td>2125.870000</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"code","source":"#Seperate dependent and independent features\nX = df_data.loc[:, df_data.columns!='Class']\ny = df_data['Class']","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:15:07.925115Z","iopub.execute_input":"2022-05-27T09:15:07.925429Z","iopub.status.idle":"2022-05-27T09:15:07.962723Z","shell.execute_reply.started":"2022-05-27T09:15:07.925398Z","shell.execute_reply":"2022-05-27T09:15:07.961282Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**SMOTE** is an abbreviation for Synthetic Minority Oversampling Technique.\n\nSMOTE works by randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.","metadata":{}},{"cell_type":"code","source":"#resampling\nsmote = SMOTE()\n# fit predictor and target variable\nX_smote, y_smote = smote.fit_resample(X,y)\nprint('Original dataset shape', Counter(y))\nprint('Resample dataset shape', Counter(y_smote))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:15:39.527630Z","iopub.execute_input":"2022-05-27T09:15:39.528432Z","iopub.status.idle":"2022-05-27T09:15:40.282127Z","shell.execute_reply.started":"2022-05-27T09:15:39.528388Z","shell.execute_reply":"2022-05-27T09:15:40.281185Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Original dataset shape Counter({0: 284315, 1: 492})\nResample dataset shape Counter({0: 284315, 1: 284315})\n","output_type":"stream"}]},{"cell_type":"code","source":"# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:18:04.421911Z","iopub.execute_input":"2022-05-27T09:18:04.422216Z","iopub.status.idle":"2022-05-27T09:18:04.531351Z","shell.execute_reply.started":"2022-05-27T09:18:04.422172Z","shell.execute_reply":"2022-05-27T09:18:04.530637Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# summarize\nprint('Train', X_train.shape, y_train.shape)\nprint('Test', X_valid.shape, y_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:18:19.891723Z","iopub.execute_input":"2022-05-27T09:18:19.892272Z","iopub.status.idle":"2022-05-27T09:18:19.900237Z","shell.execute_reply.started":"2022-05-27T09:18:19.892220Z","shell.execute_reply":"2022-05-27T09:18:19.898977Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Train (199364, 30) (199364,)\nTest (85443, 30) (85443,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Logistic Regression Model","metadata":{}},{"cell_type":"code","source":"#Build Logistic Regression Model\nlog_model = LogisticRegression(random_state=10, n_jobs=-1)\n\n#Apply Logistic Regression Model\nlog_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:22:28.580394Z","iopub.execute_input":"2022-05-27T09:22:28.580879Z","iopub.status.idle":"2022-05-27T09:22:52.456697Z","shell.execute_reply.started":"2022-05-27T09:22:28.580845Z","shell.execute_reply":"2022-05-27T09:22:52.455861Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"LogisticRegression(n_jobs=-1, random_state=10)"},"metadata":{}}]},{"cell_type":"code","source":"#make predictions on test data\ny_log = log_model.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:24:13.872908Z","iopub.execute_input":"2022-05-27T09:24:13.873266Z","iopub.status.idle":"2022-05-27T09:24:13.888863Z","shell.execute_reply.started":"2022-05-27T09:24:13.873227Z","shell.execute_reply":"2022-05-27T09:24:13.887829Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#Performance metrics\nprint('Model accuracy score: ',accuracy_score(y_valid,y_log))\nprint('Confusion matrix: ')\nprint(confusion_matrix(y_valid,y_log))\nprint(classification_report(y_valid,y_log))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:24:28.125291Z","iopub.execute_input":"2022-05-27T09:24:28.126427Z","iopub.status.idle":"2022-05-27T09:24:28.243275Z","shell.execute_reply.started":"2022-05-27T09:24:28.126369Z","shell.execute_reply":"2022-05-27T09:24:28.242584Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Model accuracy score:  0.9988998513628969\nConfusion matrix: \n[[85260    36]\n [   58    89]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     85296\n           1       0.71      0.61      0.65       147\n\n    accuracy                           1.00     85443\n   macro avg       0.86      0.80      0.83     85443\nweighted avg       1.00      1.00      1.00     85443\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### XGBoost Model","metadata":{}},{"cell_type":"code","source":"#Define hyper-parameters\n#Hyper parameter optimization\nparams={\n   \"learning_rate\": [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n    \"max_depth\": [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n    \"min_child_weight\": [1,3,5,7,9],\n    \"gamma\": [0.0, 0.1, 0.2, 0.3, 0.4],\n    \"colsample_bytree\": [0.3, 0.4, 0.5, 0.6, 0.7]\n}\n\n#Build a model\nclassifier=xgboost.XGBClassifier()\nrandom_search=RandomizedSearchCV(classifier, param_distributions=params, n_iter=5, n_jobs=-1, cv=5,verbose=0)\n\n#Apply model\nrandom_search.fit(X_train, y_train)\n\n#Get best parameters\nrandom_search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:31:57.060547Z","iopub.execute_input":"2022-05-27T09:31:57.060869Z","iopub.status.idle":"2022-05-27T09:47:17.505166Z","shell.execute_reply.started":"2022-05-27T09:31:57.060838Z","shell.execute_reply":"2022-05-27T09:47:17.504222Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"[09:46:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.4,\n              enable_categorical=False, gamma=0.1, gpu_id=-1,\n              importance_type=None, interaction_constraints='',\n              learning_rate=0.2, max_delta_step=0, max_depth=16,\n              min_child_weight=7, missing=nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=4, num_parallel_tree=1, predictor='auto',\n              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n              subsample=1, tree_method='exact', validate_parameters=1,\n              verbosity=None)"},"metadata":{}}]},{"cell_type":"code","source":"best_random = random_search.best_estimator_\n\n#Make predictions on test data\ny_xgb = best_random.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:47:47.084519Z","iopub.execute_input":"2022-05-27T09:47:47.085645Z","iopub.status.idle":"2022-05-27T09:47:47.194071Z","shell.execute_reply.started":"2022-05-27T09:47:47.085583Z","shell.execute_reply":"2022-05-27T09:47:47.193374Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#Performance metrics\nprint('Model accuracy score: ',accuracy_score(y_valid,y_xgb))\nprint('Confusion matrix: ')\nprint(confusion_matrix(y_valid,y_xgb))\nprint(classification_report(y_valid,y_xgb))","metadata":{"execution":{"iopub.status.busy":"2022-05-27T09:47:52.819220Z","iopub.execute_input":"2022-05-27T09:47:52.819674Z","iopub.status.idle":"2022-05-27T09:47:52.937813Z","shell.execute_reply.started":"2022-05-27T09:47:52.819643Z","shell.execute_reply":"2022-05-27T09:47:52.936657Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Model accuracy score:  0.9995201479348805\nConfusion matrix: \n[[85290     6]\n [   35   112]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     85296\n           1       0.95      0.76      0.85       147\n\n    accuracy                           1.00     85443\n   macro avg       0.97      0.88      0.92     85443\nweighted avg       1.00      1.00      1.00     85443\n\n[09:32:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:35:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:38:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:40:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:42:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:44:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[09:32:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:35:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:38:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:41:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:43:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:44:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:32:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:35:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:38:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:41:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:43:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:45:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[09:32:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:35:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:38:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:40:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:42:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:43:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[09:45:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}